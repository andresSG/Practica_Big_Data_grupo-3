{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import *\nimport boto3\nimport time\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::114652167878:role/AWSGlueAndS3RoleGrupo2\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 87ba3f3b-6912-456e-8eb6-f4a27f0495f7\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session 87ba3f3b-6912-456e-8eb6-f4a27f0495f7 to get into ready status...\nSession 87ba3f3b-6912-456e-8eb6-f4a27f0495f7 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "REFINED_DATABASE = \"pr2-grupo3-rodaan-refined-db\"\nS3_REFINED_BUCKET = \"pr2-grupo3-rodaan-refined-layer\"\n\nCATALOG_TABLES = {\"countries\":\"dim_countries\",\"users\":\"dim_users\",\"facts\":\"facts_user\",\"horoscopes\":\"dim_horoscopes\"}\n\nS3_CURATED_COUNTRIES = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_countries/\"\nS3_REFINED_HOROSCOPES = \"s3://pr2-grupo3-rodaan-refined-layer/data/dim_horoscopes/\"\n\nS3_REFINED_COUNTRIES = \"s3://pr2-grupo3-rodaan-refined-layer/data/dim_countries/\"\nS3_CURATED_HOROSCOPES = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_horoscopes/\"\n\nS3_CURATED_USERS = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_users/\"\nS3_REFINED_USERS = \"s3://pr2-grupo3-rodaan-refined-layer/data/dim_users/\"\n\nS3_CURATED_FACTS = \"s3://pr2-grupo3-rodaan-curated-layer/data/facts_user/\"\nS3_REFINED_FACTS = \"s3://pr2-grupo3-rodaan-refined-layer/data/facts_user/\"\n\nPATH_LOG = \"pr2-grupo3-rodaan-refined-layer/log/\"\n\ninf_VARIABLE_LECTURA_S3_COUNTRIES = \" LOG INF: Reading countries data file to dataframe\"\ninf_VARIABLE_LECTURA_S3_USERS = \" LOG INF: Reading users data file to dataframe\"\ninf_VARIABLE_LECTURA_S3_FACTS = \" LOG INF: Reading facts data file to dataframe\"\ninf_VARIABLE_LECTURA_S3_HOROSCOPES = \" LOG INF: Reading horoscopes data file to dataframe\"\n\n\n\n\ninf_VARIABLE_WRITING_DF_TO_S3 = \" LOG INF: Writing dataframe to s3\"\ninf_VARIABLE_WRITING_DF_TO_S3_OK = \" LOG INF: Writing dataframe to s3 finished OK\"\n\n\ninf_VARIABLE_TRANSFORMING_S3_HOROSCOPES = \" LOG INF: Starting horoscopes transformation\"\n\ninf_VARIABLE_RENAME_COLUMNS_HOROSCOPES = \" LOG INF: horoscopes renaming horoscope column to normalize data\"\n\ninf_VARIABLE_TRANSFORMING_S3_USERS = \" LOG INF: Starting users transformation\"\n\ninf_VARIABLE_RENAME_COLUMNS_USERS = \" LOG INF: users renaming id column to normalize data\"\ninf_VARIABLE_ADDING_HOROSCOPES_JOIN_FIELDS = \" LOG INF: users adding fields day register and month register to join with horoscopes\"\n\n\n\ninf_VARIABLE_TRANSFORM_OK_HOROSCOPES = \" LOG INF: horoscopes transform finished OK\"\n\ninf_VARIABLE_TRANSFORM_OK_USERS = \" LOG INF: users cleaning transform OK\"\n\n\n\ninf_VARIABLE_LOAD_FACTS = \" LOG INF: facts loading to refined stage\"\ninf_VARIABLE_LOAD_OK_FACTS = \" LOG INF: facts loading finished OK\"\n\ninf_VARIABLE_LECTURA_S3_OK = \" LOG INF: extracting sources finished ok\"\n\ninf_VARIABLE_ADDING_HOROSCOPES_USERS = \" LOG INF: Adding horoscopes to users\"\ninf_VARIABLE_ADDING_CONTINENT_USERS = \" LOG INF: Adding continent to users\"\ninf_VARIABLE_LOAD_USERS = \" LOG INF: users loading to refined stage\"\ninf_VARIABLE_LOAD_OK_USERS = \" LOG INF: users loading finished OK\"\n\ninf_VARIABLE_LOAD_COUNTRIES = \" LOG INF: countries loading to refined stage\"\ninf_VARIABLE_LOAD_OK_COUNTRIES = \" LOG INF: countries loading finished OK\"\n\ninf_VARIABLE_LOAD_HOROSCOPES = \" LOG INF: horoscopes loading to refined stage\"\ninf_VARIABLE_LOAD_OK_HOROSCOPES = \" LOG INF: horoscopes loading finished OK\"\n\ninf_VARIABLE_WRITING_DF_TO_S3 = \" LOG INF: Starting loading to refined stage\"\ninf_VARIABLE_WRITING_DF_TO_S3_OK = \" LOG INF: loading to refined stage finished OK\"\n\ninf_VARIABLE_CLEANING_DIRECTORY_PATH = \" LOG INF: Cleaning the directory to write the new files\"\n\nDIRECTORY_PATH_COUNTRIES = \"data/dim_countries\"\nDIRECTORY_PATH_USERS = \"data/dim_users\"\nDIRECTORY_PATH_HOROSCOPES = \"data/dim_horoscopes\"\nDIRECTORY_PATH_FACTS = \"data/facts_user\"\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para obtener la timestamp en formato iso",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def get_timestamp():\n    try:\n        timestamp = time.time()\n        fecha_hora_iso = time.strftime('%Y-%m-%dT%H:%M:%S%z', time.localtime(timestamp))\n        return fecha_hora_iso\n    except ValueError:\n        return ValueError",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para vaciar una ruta de s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def vaciar_carpeta_s3(bucket, carpeta):\n    \"\"\"\n    Vacía una carpeta de un bucket de Amazon S3.\n\n    Parámetros:\n    - bucket: el nombre del bucket de S3\n    - carpeta: el nombre de la carpeta que se desea vaciar (debe estar en el formato \"carpeta/subcarpeta/\")\n\n    \"\"\"\n\n    s3 = boto3.resource('s3')\n    bucket = s3.Bucket(bucket)\n\n    for obj in bucket.objects.filter(Prefix=carpeta):\n        obj.delete()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para escribir en ficheros de log en s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def write_log_in_s3(path, texto, timestamp):\n    path = path + timestamp + \".txt\"\n    prefix = \"(\" + str(get_timestamp()) + \")\"\n    texto = prefix + texto + '\\n'\n    s3 = boto3.client('s3')\n    bucket_name, object_key = path.split('/', 1)\n    try:\n        obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n        contenido_actual = obj['Body'].read().decode('utf-8')\n        contenido_nuevo = contenido_actual + texto\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=contenido_nuevo)\n    except s3.exceptions.NoSuchKey:\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=texto)\n    else:\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=contenido_nuevo)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para guardar un df en s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def write_df_to_s3(log_timestamp, directory_path, s3_path, glueContext, spark, catalog_database, catalog_table, df_to_write, partition_keys=[]):\n    try:\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_WRITING_DF_TO_S3, log_timestamp)\n        s3output = glueContext.getSink(\n          path=s3_path,\n          connection_type=\"s3\",\n          updateBehavior=\"UPDATE_IN_DATABASE\",\n          partitionKeys= partition_keys,\n          compression=\"snappy\",\n          enableUpdateCatalog=True,\n          transformation_ctx=\"s3output\",\n        )\n        s3output.setCatalogInfo(\n          catalogDatabase= catalog_database , catalogTableName= catalog_table\n        )\n        dynamic_frame = glueContext.create_dynamic_frame_from_rdd(df_to_write.rdd, \"dynamic_frame\")\n        s3output.setFormat(\"glueparquet\")\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_CLEANING_DIRECTORY_PATH, log_timestamp)\n        vaciar_carpeta_s3(S3_REFINED_BUCKET, directory_path)\n        s3output.writeFrame(dynamic_frame)\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_WRITING_DF_TO_S3_OK, log_timestamp)\n    except Exception as e:\n        error = \"LOG: ERROR: \" + str(e)\n        write_log_in_s3(PATH_LOG, error, log_timestamp)\n        raise e \n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para leer un fichero en s3 a un df",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def read_s3_csv_to_df(log_timestamp, spark, s3_path, format = 'csv', header = 'true', delimiter = '\\t'):\n    try:\n        df_to_return = (\n            spark.read\n            .format(format)\n            .option(\"header\", header)\n            .option(\"delimiter\",delimiter)\n            .load(s3_path) \n        )\n        return df_to_return\n    except Exception as e:\n        error = \"LOG: ERROR: \" + str(e)\n        write_log_in_s3(PATH_LOG, error, log_timestamp)\n        raise e ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Extraer de la capa curated la informacion",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def extract_data(log_timestamp, spark):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_USERS, log_timestamp)\n    df_dim_users = read_s3_csv_to_df(log_timestamp, spark, S3_CURATED_USERS, 'parquet')\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_HOROSCOPES, log_timestamp)\n    df_dim_horoscopes = read_s3_csv_to_df(log_timestamp, spark, S3_CURATED_HOROSCOPES, 'parquet')\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_FACTS, log_timestamp)\n    df_dim_facts = read_s3_csv_to_df(log_timestamp, spark, S3_CURATED_FACTS, 'parquet')\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_COUNTRIES, log_timestamp)\n    df_dim_contries = read_s3_csv_to_df(log_timestamp, spark, S3_CURATED_COUNTRIES, 'parquet')\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_OK, log_timestamp)\n    \n    return df_dim_users, df_dim_horoscopes, df_dim_facts, df_dim_contries\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Generar ficheros con usuarios con sus repectivos horoscopos",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def day(date):\n    date_str = str(date)\n    date_split = date_str.split(\"-\")\n    day = int(date_split[2])\n    return day",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def transform_users(df_dim_users, df_dim_horoscopes, df_dim_contries, log_timestamp):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_TRANSFORMING_S3_USERS, log_timestamp)\n    try:\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_RENAME_COLUMNS_USERS, log_timestamp)\n        df_dim_users_rename_id = (\n            df_dim_users\n            .withColumnRenamed(\"#id\",\"user_id\")\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_ADDING_HOROSCOPES_JOIN_FIELDS, log_timestamp)\n        dayUDF = udf(lambda date: day(date))\n\n        df_dim_users_month_day_register = (\n        df_dim_users_rename_id\n            .withColumn(\"register_month\", month(\"register_date\"))\n            .withColumn(\"register_day\", dayUDF(\"register_date\"))    \n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_ADDING_HOROSCOPES_USERS, log_timestamp)\n        df_users_join_horoscopes = (\n            df_dim_users_month_day_register\n            .join(df_dim_horoscopes, (df_dim_users_month_day_register.register_day == df_dim_horoscopes.Day) & (df_dim_users_month_day_register.register_month == df_dim_horoscopes.Month),\"left\")\n            .drop(\"Date\",\"Day\",\"Month\")\n            .withColumnRenamed(\"Horocope\",\"horoscope\")  \n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_ADDING_CONTINENT_USERS, log_timestamp)\n        df_dim_users_refined = (\n            df_users_join_horoscopes\n            .join(df_dim_countries, df_users_join_horoscopes.country_name == df_dim_countries.country, \"left\")\n            .drop(df_dim_countries.country)\n            .drop(\"nombre\",\"continente\",\"pais\")\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_TRANSFORM_OK_USERS, log_timestamp)\n    except Exception as e:\n        write_log_in_s3(PATH_LOG, str(e), log_timestamp)\n        raise e \n    return df_dim_users_refined\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Generar horoscopos en refined",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def transform_horoscopes(df_dim_horoscopes, log_timestamp):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_TRANSFORMING_S3_HOROSCOPES, log_timestamp)\n    try:\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_RENAME_COLUMNS_HOROSCOPES, log_timestamp)\n        df_dim_horoscopes_refined = df_dim_horoscopes.withColumnRenamed(\"Horoscopes\",\"horoscopes\")\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_TRANSFORM_OK_HOROSCOPES, log_timestamp)\n    except Exception as e:\n        write_log_in_s3(PATH_LOG, str(e), log_timestamp)\n        raise e \n    return df_dim_horoscopes_refined\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Load data in refined layer",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def load_refined_data(log_timestamp, glueContext, spark, df_dim_users_refined, df_dim_horoscopes_refined, df_dim_countries_refined, df_dim_facts_refined):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_WRITING_DF_TO_S3, log_timestamp)\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_COUNTRIES, log_timestamp)\n    write_df_to_s3(log_timestamp, DIRECTORY_PATH_COUNTRIES, S3_REFINED_COUNTRIES, glueContext, spark, REFINED_DATABASE, CATALOG_TABLES[\"countries\"], df_dim_countries_refined)\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_OK_COUNTRIES, log_timestamp)\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_HOROSCOPES, log_timestamp)\n    write_df_to_s3(log_timestamp, DIRECTORY_PATH_HOROSCOPES,S3_REFINED_HOROSCOPES, glueContext, spark, REFINED_DATABASE, CATALOG_TABLES[\"horoscopes\"], df_dim_horoscopes_refined)\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_OK_HOROSCOPES, log_timestamp)\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_USERS, log_timestamp)\n    write_df_to_s3(log_timestamp, DIRECTORY_PATH_USERS,S3_REFINED_USERS, glueContext, spark, REFINED_DATABASE, CATALOG_TABLES[\"users\"], df_dim_users_refined)\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_OK_USERS, log_timestamp)\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_FACTS, log_timestamp)\n    write_df_to_s3(log_timestamp, DIRECTORY_PATH_FACTS,S3_REFINED_FACTS, glueContext, spark, REFINED_DATABASE, CATALOG_TABLES[\"facts\"], df_dim_facts_refined, partition_keys=[\"user_id\"])\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LOAD_OK_FACTS, log_timestamp)\n    \n    write_log_in_s3(PATH_LOG,inf_VARIABLE_WRITING_DF_TO_S3_OK, log_timestamp)\n    \n    \n    \n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### MAIN",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "timestamp = str(get_timestamp())\n\ndf_dim_users, df_dim_horoscopes, df_dim_facts, df_dim_countries = extract_data(timestamp, spark)\n\ndf_dim_users_refined = transform_users(df_dim_users, df_dim_horoscopes, df_dim_countries, timestamp)\ndf_dim_horoscopes_refined = transform_horoscopes(df_dim_horoscopes, timestamp)\n\nload_refined_data(timestamp, glueContext, spark, df_dim_users_refined, df_dim_horoscopes_refined, df_dim_countries, df_dim_facts)\n\n\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+\n|    user_id|gender| age|           country|  registered|        country_name|register_date|register_month|register_day|Horoscope|\n+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+\n|user_000001|     m|null|             Japan|Aug 13, 2006|               Japan|   2006-08-13|             8|          13|      Leo|\n|user_000002|     f|null|              Peru|Feb 24, 2006|                Peru|   2006-02-24|             2|          24|   Pisces|\n|user_000003|     m|  22|     United States|Oct 30, 2005|United States of ...|   2005-10-30|            10|          30|  Scorpio|\n|user_000004|     f|null|              null|Apr 26, 2006|                null|   2006-04-26|             4|          26|   Taurus|\n|user_000005|     m|null|          Bulgaria|Jun 29, 2006|            Bulgaria|   2006-06-29|             6|          29|   Cancer|\n|user_000006|  null|  24|Russian Federation|May 18, 2006|              Russia|   2006-05-18|             5|          18|   Taurus|\n|user_000007|     f|null|     United States|Jan 22, 2006|United States of ...|   2006-01-22|             1|          22| Aquarius|\n|user_000008|     m|  23|          Slovakia|Sep 28, 2006|            Slovakia|   2006-09-28|             9|          28|    Libra|\n|user_000009|     f|  19|     United States|Jan 13, 2007|United States of ...|   2007-01-13|             1|          13|Capricorn|\n|user_000010|     m|  19|            Poland| May 4, 2006|              Poland|   2006-05-04|             5|           4|   Taurus|\n|user_000011|     m|  21|           Finland| Sep 8, 2005|             Finland|   2005-09-08|             9|           8|    Virgo|\n|user_000012|     f|  28|     United States|Mar 30, 2005|United States of ...|   2005-03-30|             3|          30|    Aries|\n|user_000013|     f|  25|           Romania|Sep 25, 2006|             Romania|   2006-09-25|             9|          25|    Libra|\n|user_000014|  null|null|              null|Jan 27, 2006|                null|   2006-01-27|             1|          27| Aquarius|\n|user_000015|  null|  21|     Cote D'Ivoire| Oct 3, 2006|       Cote D'Ivoire|   2006-10-03|            10|           3|    Libra|\n|user_000016|     m|null|    United Kingdom| Aug 5, 2005|      United Kingdom|   2005-08-05|             8|           5|      Leo|\n|user_000017|     m|  22|           Morocco|Aug 27, 2007|             Morocco|   2007-08-27|             8|          27|    Virgo|\n|user_000018|  null|  22|    United Kingdom|Aug 26, 2005|      United Kingdom|   2005-08-26|             8|          26|    Virgo|\n|user_000019|     f|  29|            Mexico|Nov 10, 2005|              Mexico|   2005-11-10|            11|          10|  Scorpio|\n|user_000020|     f|  27|           Germany|Jul 24, 2006|             Germany|   2006-07-24|             7|          24|      Leo|\n+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+\nonly showing top 20 rows\n\n+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+---------+\n|    user_id|gender| age|           country|  registered|        country_name|register_date|register_month|register_day|Horoscope|continent|\n+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+---------+\n|user_000001|     m|null|             Japan|Aug 13, 2006|               Japan|   2006-08-13|             8|          13|      Leo|     Asia|\n|user_000002|     f|null|              Peru|Feb 24, 2006|                Peru|   2006-02-24|             2|          24|   Pisces|  America|\n|user_000003|     m|  22|     United States|Oct 30, 2005|United States of ...|   2005-10-30|            10|          30|  Scorpio|  America|\n|user_000004|     f|null|              null|Apr 26, 2006|                null|   2006-04-26|             4|          26|   Taurus|     null|\n|user_000005|     m|null|          Bulgaria|Jun 29, 2006|            Bulgaria|   2006-06-29|             6|          29|   Cancer|   Europe|\n|user_000006|  null|  24|Russian Federation|May 18, 2006|              Russia|   2006-05-18|             5|          18|   Taurus|   Europe|\n|user_000007|     f|null|     United States|Jan 22, 2006|United States of ...|   2006-01-22|             1|          22| Aquarius|  America|\n|user_000008|     m|  23|          Slovakia|Sep 28, 2006|            Slovakia|   2006-09-28|             9|          28|    Libra|   Europe|\n|user_000009|     f|  19|     United States|Jan 13, 2007|United States of ...|   2007-01-13|             1|          13|Capricorn|  America|\n|user_000010|     m|  19|            Poland| May 4, 2006|              Poland|   2006-05-04|             5|           4|   Taurus|   Europe|\n|user_000011|     m|  21|           Finland| Sep 8, 2005|             Finland|   2005-09-08|             9|           8|    Virgo|   Europe|\n|user_000012|     f|  28|     United States|Mar 30, 2005|United States of ...|   2005-03-30|             3|          30|    Aries|  America|\n|user_000013|     f|  25|           Romania|Sep 25, 2006|             Romania|   2006-09-25|             9|          25|    Libra|   Europe|\n|user_000014|  null|null|              null|Jan 27, 2006|                null|   2006-01-27|             1|          27| Aquarius|     null|\n|user_000015|  null|  21|     Cote D'Ivoire| Oct 3, 2006|       Cote D'Ivoire|   2006-10-03|            10|           3|    Libra|   Africa|\n|user_000016|     m|null|    United Kingdom| Aug 5, 2005|      United Kingdom|   2005-08-05|             8|           5|      Leo|   Europe|\n|user_000017|     m|  22|           Morocco|Aug 27, 2007|             Morocco|   2007-08-27|             8|          27|    Virgo|   Africa|\n|user_000018|  null|  22|    United Kingdom|Aug 26, 2005|      United Kingdom|   2005-08-26|             8|          26|    Virgo|   Europe|\n|user_000019|     f|  29|            Mexico|Nov 10, 2005|              Mexico|   2005-11-10|            11|          10|  Scorpio|  America|\n|user_000020|     f|  27|           Germany|Jul 24, 2006|             Germany|   2006-07-24|             7|          24|      Leo|   Europe|\n+-----------+------+----+------------------+------------+--------------------+-------------+--------------+------------+---------+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}