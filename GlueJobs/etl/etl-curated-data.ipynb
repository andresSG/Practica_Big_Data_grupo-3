{
	"metadata": {
		"toc-autonumbering": true,
		"toc-showmarkdowntxt": true,
		"toc-showcode": true,
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import *\nimport boto3\nimport time\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::114652167878:role/AWSGlueAndS3RoleGrupo2\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: a633a979-f804-40eb-8817-35c5f22f4def\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session a633a979-f804-40eb-8817-35c5f22f4def to get into ready status...\nSession a633a979-f804-40eb-8817-35c5f22f4def has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "CURATED_DATABASE = \"pr2-grupo3-rodaan-curated\"\nS3_CURATED_BUCKET = \"pr2-grupo3-rodaan-curated-layer\"\n\nCATALOG_TABLES = {\"countries\":\"dim_countries\",\"users\":\"dim_users\",\"facts\":\"facts_user\",\"horoscopes\":\"dim_horoscopes\"}\nS3_CURATED_COUNTRIES = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_countries/\"\nS3_CURATED_HOROSCOPES = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_horoscopes/\"\nS3_RAW_COUNTRIES = \"s3://pr2-grupo3-rodaan-raw-layer/data/country/paises.csv\"\nS3_RAW_HOROSCOPES = \"s3://pr2-grupo3-rodaan-raw-layer/data/horoscope/Horoscope.csv\"\n\nS3_RAW_USERS = \"s3://pr2-grupo3-rodaan-raw-layer/data/user/userid-profile.tsv\"\nS3_CURATED_USERS = \"s3://pr2-grupo3-rodaan-curated-layer/data/dim_users/\"\n\nS3_RAW_FACTS = \"s3://pr2-grupo3-rodaan-raw-layer/data/user/userid-timestamp-artid-artname-traid-traname.tsv\"\nS3_CURATED_FACTS = \"s3://pr2-grupo3-rodaan-curated-layer/data/facts_user/\"\n\nPATH_LOG = \"pr2-grupo3-rodaan-curated-layer/log/\"\ninf_VARIABLE_LECTURA_S3_COUNTRIES = \" LOG INF: Reading countries data file to dataframe\"\ninf_VARIABLE_CAMBIO_NOMBRES = \" LOG INF: Changing column names\"\ninf_VARIABLE_GENERANDO_CONTINENTES_INGLES = \" LOG INF: Creating column continent in english\"\ninf_VARIABLE_GENERANDO_PAISES_NUEVOS = \" LOG INF: Adding country rows to dataframe\"\ninf_VARIABLE_ELIMINAR_PAISES_DUPLICADOS = \" LOG INF: Removing congo duplicates from dataframe\"\ninf_VARIABLE_CAMBIANDO_CONTINENTES = \" LOG INF: Updating continent in countries that don't have continent\"\ninf_VARIABLE_WRITING_DF_TO_S3 = \" LOG INF: Writing dataframe to s3\"\ninf_VARIABLE_WRITING_DF_TO_S3_OK = \" LOG INF: Writing dataframe to s3 finished OK\"\ninf_VARIABLE_CLEANING_OK_COUNTRIES = \" LOG INF: Countries cleaning finished OK\"\n\ninf_VARIABLE_LECTURA_S3_HOROSCOPÈS = \" LOG INF: Reading horoscopes data file to dataframe\"\ninf_VARIABLE_CLEANING_OK_HOROSCOPES = \" LOG INF: Horoscopes cleaning finished OK\"\n\ninf_VARIABLE_LECTURA_S3_USERS = \" LOG INF: Reading users data file to dataframe\"\ninf_VARIABLE_CLEANING_OK_USERS = \" LOG INF: users cleaning finished OK\"\ninf_VARIABLE_CAMPO_CRUCE_PAIS_USERS = \" LOG INF: users creating a column with the same country values as dim_countries to join\"\ninf_VARIABLE_FORMAT_DATE_USERS = \" LOG INF: users creating a column with the correct date format for register date\"\ninf_VARIABLE_CLEANING_OK_USERS = \" LOG INF: Users cleaning finished OK\"\n\ninf_VARIABLE_LECTURA_S3_FACTS = \" LOG INF: Reading facts data file to dataframe\"\ninf_VARIABLE_CLEANING_OK_FACTS = \" LOG INF: facts cleaning finished OK\"\ninf_VARIABLE_COLUMNAS_FACTS = \" LOG INF: facts giving the correct name to its columns\"\ninf_VARIABLE_TIMESTAMPS_FACTS = \" LOG INF: facts giving the correct format and column type to the fact timestamp\"\ninf_VARIABLE_CLEANING_NON_VALUE_ROWS_FACTS = \" LOG INF: facts dropping rows without values in all columns\"\ninf_VARIABLE_CLEANING_OK_FACTS = \" LOG INF: facts cleaning finished OK\"\n\ninf_VARIABLE_CLEANING_DIRECTORY_PATH = \" LOG INF: Cleaning the directory to write the new files\"\n\nDIRECTORY_PATH_COUNTRIES = \"data/dim_countries\"\nDIRECTORY_PATH_USERS = \"data/dim_users\"\nDIRECTORY_PATH_HOROSCOPES = \"data/dim_horoscopes\"\nDIRECTORY_PATH_FACTS = \"data/facts_user\"\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 73,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para obtener la timestamp en formato iso",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def get_timestamp():\n    try:\n        timestamp = time.time()\n        fecha_hora_iso = time.strftime('%Y-%m-%dT%H:%M:%S%z', time.localtime(timestamp))\n        return fecha_hora_iso\n    except ValueError:\n        return ValueError",
			"metadata": {
				"trusted": true
			},
			"execution_count": 74,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para vaciar una ruta de s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def vaciar_carpeta_s3(bucket, carpeta):\n    \"\"\"\n    Vacía una carpeta de un bucket de Amazon S3.\n\n    Parámetros:\n    - bucket: el nombre del bucket de S3\n    - carpeta: el nombre de la carpeta que se desea vaciar (debe estar en el formato \"carpeta/subcarpeta/\")\n\n    \"\"\"\n\n    s3 = boto3.resource('s3')\n    bucket = s3.Bucket(bucket)\n\n    for obj in bucket.objects.filter(Prefix=carpeta):\n        obj.delete()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 75,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para escribir en ficheros de log en s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def write_log_in_s3(path, texto, timestamp):\n    path = path + timestamp + \".txt\"\n    prefix = \"(\" + str(get_timestamp()) + \")\"\n    texto = prefix + texto + '\\n'\n    s3 = boto3.client('s3')\n    bucket_name, object_key = path.split('/', 1)\n    try:\n        obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n        contenido_actual = obj['Body'].read().decode('utf-8')\n        contenido_nuevo = contenido_actual + texto\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=contenido_nuevo)\n    except s3.exceptions.NoSuchKey:\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=texto)\n    else:\n        s3.put_object(Bucket=bucket_name, Key=object_key, Body=contenido_nuevo)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 76,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para guardar un df en s3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def write_df_to_s3(log_timestamp, directory_path, s3_path, glueContext, spark, catalog_database, catalog_table, df_to_write, partition_keys=[]):\n    try:\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_WRITING_DF_TO_S3, log_timestamp)\n        s3output = glueContext.getSink(\n          path=s3_path,\n          connection_type=\"s3\",\n          updateBehavior=\"UPDATE_IN_DATABASE\",\n          partitionKeys= partition_keys,\n          compression=\"snappy\",\n          enableUpdateCatalog=True,\n          transformation_ctx=\"s3output\",\n        )\n        s3output.setCatalogInfo(\n          catalogDatabase= catalog_database , catalogTableName= catalog_table\n        )\n        dynamic_frame = glueContext.create_dynamic_frame_from_rdd(df_to_write.rdd, \"dynamic_frame\")\n        s3output.setFormat(\"glueparquet\")\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_CLEANING_DIRECTORY_PATH, log_timestamp)\n        vaciar_carpeta_s3(S3_CURATED_BUCKET, directory_path)\n        s3output.writeFrame(dynamic_frame)\n        write_log_in_s3(PATH_LOG, inf_VARIABLE_WRITING_DF_TO_S3_OK, log_timestamp)\n    except Exception as e:\n        error = \"LOG: ERROR: \" + str(e)\n        write_log_in_s3(PATH_LOG, error, log_timestamp)\n        raise e \n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 77,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Funcion para leer un fichero en s3 a un df",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def read_s3_csv_to_df(log_timestamp, spark, s3_path, format = 'csv', header = 'true', delimiter = '\\t'):\n    try:\n        df_to_return = (\n            spark.read\n            .format(format)\n            .option(\"header\", header)\n            .option(\"delimiter\",delimiter)\n            .load(s3_path) \n        )\n        return df_to_return\n    except Exception as e:\n        error = \"LOG: ERROR: \" + str(e)\n        write_log_in_s3(PATH_LOG, error, log_timestamp)\n        raise e ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 78,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Limpiar fichero de countries",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def clean_countries(spark, s3_path, glueContext, log_timestamp):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_COUNTRIES, log_timestamp)\n    df_countries = read_s3_csv_to_df(log_timestamp, spark, S3_RAW_COUNTRIES, 'csv', 'true', delimiter = ',')\n    ##limpieza de los nombres originales\n    try:\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CAMBIO_NOMBRES, log_timestamp)\n        df_countries_refined_names = (\n            df_countries\n            .withColumnRenamed(' name', \"country\")\n            .withColumnRenamed(' iso2', 'iso2')\n            .withColumnRenamed(' iso3', 'iso3')\n            .withColumnRenamed(' nom', 'nom')\n            .withColumnRenamed(' phone_code', 'phone_code')\n        )\n        ##incluir campos de continentes en ingles\n        ##el dataset original los tenia en español\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_GENERANDO_CONTINENTES_INGLES, log_timestamp)\n        df_countries_english_names = (\n            df_countries_refined_names\n            .withColumn(\"continent\",when( expr(\"continente == 'Europa'\"), \"Europe\"  )\n                       .when( expr(\"continente == 'África'\"), \"Africa\"  )\n                       .when( expr(\"continente == 'Australia y Oceanía'\"), \"Australia and Oceania\"  )\n                       .when( expr(\"continente == 'América'\"), \"America\"  )\n                       .when( expr(\"continente == 'Antártida'\"), \"Antarctica\"  )\n                       .otherwise( col(\"continente\") ))\n            .withColumnRenamed(\"nombre\",\"pais\")\n        )\n        \"\"\"##Incluir costa de marfil en frances, el data set original no lo tenia\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_GENERANDO_PAISES_NUEVOS, log_timestamp)\n        df_countries_reduce_rows = (\n            df_countries_english_names\n            .drop(\"iso2\",\"iso3\",\"phone_code\",\"nom\")\n        )\n\n        df_new_row = spark.createDataFrame([[\"Cote D'Ivoire\",\"Cote D'Ivoire\",\"África\",\"Africa\"]])\n\n        df_countries_union = df_countries_reduce_rows.union(df_new_row)\"\"\"\n        ##El congo viene duplicado en el data set original\n        ##Se puede eliminar una de las dos filas ya que no aporta ningun valor añadido extra\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_ELIMINAR_PAISES_DUPLICADOS, log_timestamp)\n        df_countries_drop_duplicates = (\n            df_countries_english_names\n            .dropDuplicates()\n            .drop(\"iso2\",\"iso3\",\"phone_code\",\"nom\")\n\n        )\n        ##Como se vio en la parte de analitycs, hay paises con continente a null\n        ##Estos paises son archipielagos de islas que realmente no pertenecen a ningun continente\n        ##Para diferenciar el valor null (no hay dato) del valor no tiene continente se cambia el valor de estos registros\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CAMBIANDO_CONTINENTES, log_timestamp)\n        df_countries_curated = (\n        df_countries_drop_duplicates\n            .withColumn(\"continente\",\n                        when( expr(\"continente is NULL\"),\"Sin continente\" )\n                        .otherwise(col(\"continente\")) )\n            .withColumn(\"continent\",\n                        when( expr(\"continent is NULL\"),\"Without continent\" )\n                        .otherwise(col(\"continent\")) )\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_COUNTRIES, log_timestamp)\n    except Exception as e:\n        write_log_in_s3(PATH_LOG, str(e), log_timestamp)\n        raise e \n    return df_countries_curated\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 79,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Limpiar horoscopos",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def extract_horoscopes(spark, s3_path, glueContext, log_timestamp):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_HOROSCOPÈS, log_timestamp)\n    df_horoscopes = read_s3_csv_to_df(log_timestamp, spark, S3_RAW_HOROSCOPES, 'csv', 'true', delimiter = ',')\n    return df_horoscopes",
			"metadata": {
				"trusted": true
			},
			"execution_count": 80,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Limpiar usuarios",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "#### Funcion para generar una udf para convertir los formatos de fecha",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def udf_date_convert(date):\n    def right(s, amount):\n        return s[-amount:]\n    date_split = date.split(\" \")\n    new_date_list = []\n    for i,value in enumerate(date_split):\n        if i == 1:\n            \n            row = '0' + value.replace(\",\",\"\")\n            print(row)\n            row = right(row,2)\n            new_date_list.append(row)\n        \n        else:\n            new_date_list.append(value)  \n    if new_date_list[0] == 'Jan':\n        value = '01'\n    elif new_date_list[0] == 'Feb':\n        value = '02'\n    elif new_date_list[0] == 'Mar':\n        value = '03'\n    elif new_date_list[0] == 'Apr':\n        value = '04'\n    elif new_date_list[0] == 'May':\n        value = '05'\n    elif new_date_list[0] == 'Jun':\n        value = '06'\n    elif new_date_list[0] == 'Jul':\n        value = '07'\n    elif new_date_list[0] == 'Aug':\n        value = '08'\n    elif new_date_list[0] == 'Sep':\n        value = '09'\n    elif new_date_list[0] == 'Oct':\n       value = '10'\n    elif new_date_list[0] == 'Nov':\n        value = '11'\n    elif new_date_list[0] == 'Dec':\n        value = '12'\n    else:\n        value = '00'\n    return new_date_list[1] + '-' + str(value) + '-' + new_date_list[2]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 81,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def clean_users(spark, s3_path, glueContext, log_timestamp):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_USERS, log_timestamp)\n    df_users = read_s3_csv_to_df(log_timestamp, spark, S3_RAW_USERS, 'csv', 'true', delimiter = \"\\t\")\n    try:\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CAMPO_CRUCE_PAIS_USERS, log_timestamp)\n        df_users_refined = (\n            df_users\n            .withColumn(\"country_name\", when( expr(\"country like 'Congo%'\"),\"Congo\" )\n                        .when( expr(\"country like 'Korea, Democratic%'\"),\"South Korea\" )\n                        .when( expr(\"country like 'Russian%'\"),\"Russia\" )\n                        .when( expr(\"country like 'United States%'\"),\"United States of America\" )\n                        .when( expr(\"country like 'Cote D%Ivoire%'\"),\"Ivory Coast\" )\n                        .otherwise(col(\"country\")) )\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_FORMAT_DATE_USERS, log_timestamp)\n        conversion = udf(lambda z: udf_date_convert(z))      \n        df_users_date_conversion = df_users_refined.where(col(\"registered\").isNotNull()).withColumn(\"register_date\",conversion(col(\"registered\")) )\n        df_users_curated = df_users_date_conversion.withColumn(\"registered_date\", to_date(col(\"register_date\"),\"dd-MM-yyyy\")) \n        df_users_curated_final = df_users_curated.drop(\"register_date\").withColumnRenamed(\"registered_date\",\"register_date\")\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CLEANING_OK_USERS, log_timestamp)\n    except Exception as e:\n        write_log_in_s3(PATH_LOG, str(e), log_timestamp)\n        raise e \n    return df_users_curated_final",
			"metadata": {
				"trusted": true
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Limpiar hechos de usuarios",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def generate_list_users_to_clean(df_users_without_values):\n    user_list_withtout_values = (\n        df_users_without_values\n        .select(\"#id\")\n        .collect()\n    )\n    list_users_to_clean = []\n    for row in user_list_withtout_values:\n       list_users_to_clean.append((row.asDict()[\"user_id\"]))\n    return list_users_to_clean",
			"metadata": {
				"trusted": true
			},
			"execution_count": 83,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def clean_facts(spark, s3_path, glueContext, log_timestamp, df_users_curated):\n    write_log_in_s3(PATH_LOG,inf_VARIABLE_LECTURA_S3_USERS, log_timestamp)\n    df_facts = read_s3_csv_to_df(log_timestamp, spark, S3_RAW_FACTS, 'csv', 'false', delimiter = \"\\t\")\n    try:\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_COLUMNAS_FACTS, log_timestamp)\n        df_user_facts_renamed = (\n            df_facts\n            .withColumnRenamed(\"_c1\",\"fact_date\")\n            .withColumnRenamed(\"_c0\",\"user_id\")\n            .withColumnRenamed(\"_c3\",\"group_name\")\n            .withColumnRenamed(\"_c2\",\"group_id\")\n            .withColumnRenamed(\"_c5\",\"song_name\")\n            .withColumnRenamed(\"_c4\",\"song_id\")\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_TIMESTAMPS_FACTS, log_timestamp)\n        df_user_facts_formatted_date = (\n            df_user_facts_renamed\n            .select(\"*\", to_timestamp(\"fact_date\").alias(\"fact_timestamp\"))\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CLEANING_NON_VALUE_ROWS_FACTS, log_timestamp)\n        df_users_without_values = df_users_curated.where(\"registered is null\")\n        list_users_to_clean = generate_list_users_to_clean(df_users_without_values)\n        df_filter_facts_with_users_not_valid = (\n            df_user_facts_formatted_date\n            .where(~col(\"user_id\").isin(list_users_to_clean))\n        )\n        write_log_in_s3(PATH_LOG,inf_VARIABLE_CLEANING_OK_FACTS, log_timestamp)\n    except Exception as e:\n        write_log_in_s3(PATH_LOG, str(e), log_timestamp)\n        raise e \n    return df_filter_facts_with_users_not_valid\n    \n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 84,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### MAIN",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "timestamp = str(get_timestamp())\n\ndf_countries_curated = clean_countries(spark, S3_RAW_COUNTRIES, glueContext,timestamp)\nwrite_df_to_s3(timestamp, DIRECTORY_PATH_COUNTRIES, S3_CURATED_COUNTRIES, glueContext, spark, CURATED_DATABASE, CATALOG_TABLES[\"countries\"], df_countries_curated)\n\n\ndf_horoscopes_curated = extract_horoscopes(spark, S3_RAW_HOROSCOPES, glueContext,timestamp)\nwrite_df_to_s3(timestamp, DIRECTORY_PATH_HOROSCOPES,S3_CURATED_HOROSCOPES, glueContext, spark, CURATED_DATABASE, CATALOG_TABLES[\"horoscopes\"], df_horoscopes_curated)\n\ndf_users_curated = clean_users(spark, S3_RAW_USERS, glueContext,timestamp)\nwrite_df_to_s3(timestamp, DIRECTORY_PATH_USERS,S3_CURATED_USERS, glueContext, spark, CURATED_DATABASE, CATALOG_TABLES[\"users\"], df_users_curated)\n\ndf_facts_curated = clean_facts(spark, S3_RAW_FACTS, glueContext, timestamp, df_users_curated)\nwrite_df_to_s3(timestamp, DIRECTORY_PATH_FACTS,S3_CURATED_FACTS, glueContext, spark, CURATED_DATABASE, CATALOG_TABLES[\"facts\"], df_facts_curated, partition_keys=[\"user_id\"])\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}